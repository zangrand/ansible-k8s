apiVersion: "sparkoperator.k8s.io/v1beta1"
kind: SparkApplication
metadata:
  name: streamprocessor
  namespace: default
spec:
  type: Scala
  mode: cluster
  image: "gcr.io/spark-operator/spark:v2.4.0"
  imagePullPolicy: Always
  mainClass: ch.cern.StreamProcessor
  mainApplicationFile: "http://www.pd.infn.it/~verlato/StreamProcessing-1.0-SNAPSHOT.jar"
  arguments: 
    - "produce28partitions"
    - "occupancyplot"
    - "eventsdataframe"
    - "8000"
    - "4"
  sparkConf:
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer"
    "spark.streaming.stopGracefullyOnShutdown": "true"
    "spark.streaming.unpersist": "true"
    "spark.ui.retainedJobs": "100"
    "spark.ui.retainedStages": "100"
    "spark.ui.retainedTasks": "100"
    "spark.worker.ui.retainedExecutors": "100"
    "spark.worker.ui.retainedDrivers": "100"
    "spark.sql.ui.retainedExecutions": "100"
    "spark.streaming.ui.retainedBatches": "100"
    "spark.dynamicAllocation.enabled": "false"
    "spark.cleaner.referenceTracking.cleanCheckpoints": "true"
    "spark.cleaner.referenceTracking.blocking": "false"
    "spark.driver.extraJavaOptions": "-XX:+UseConcMarkSweepGC"
    "spark.executor.extraJavaOptions": "-XX:+UseConcMarkSweepGC"
    "spark.jars.packages": "org.apache.spark:spark-streaming-kafka-0-10_2.11:2.3.2,org.apache.spark:spark-streaming_2.11:2.3.2,org.apache.kafka:kafka_2.11:2.0.0,org.apache.kafka:kafka-clients:2.0.0"
  restartPolicy:
    type: Never
  driver:
    cores: 0.1
    coreLimit: "200m"
    memory: "512m"
    labels:
      version: 2.4.0
  executor:
    cores: 1
    instances: 1
    memory: "512m"
    labels:
      version: 2.4.0
